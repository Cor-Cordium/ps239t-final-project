---
title: "Final Project I: Using Textual Analysis to analyze three collections of poetry"
author: "Cory Merrill"
date: "November 17, 2015"
output: html_document
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
rm(list=ls())
setwd("/Users/Orly/ps239t-final-project")
library(tm) # loading recommended libraries
library(RTextTools) 
library(qdap)
library(qdapDictionaries)
library(dplyr) 
library(ggplot2) 
library(SnowballC) 
library(entropy)

getSources()
getReaders()

Pound <- Corpus(DirSource("Data/Pound"))
Eliot <- Corpus(DirSource("Data/TS_Eliot"))
HD <- Corpus(DirSource("Data/HD"))

#as.character(Pound)
#as.character(Eliot)
#as.character(HD)


getTransformations()

#Clean data for Pound collection
Pound <- tm_map(Pound, content_transformer(tolower)) # convert all text to lower case
#as.character(Pound)
Pound <- tm_map(Pound, removePunctuation) # remove Punctuation
#as.character(Pound)
Pound <- tm_map(Pound, removeNumbers) # remove Numbers
#as.character(Pound)
Pound <- tm_map(Pound, removeWords, stopwords("english")) # remove common words
#stopwords("english") # check out what was removed
Pound <- tm_map(Pound, removeWords, c("gutenbergtm", "project", "gutenberg", "electron", "distribut", "foundat", "copyright", "access", "licen", "donat", "ebook", "copi", "archiv", "work")) #I removed terms that unrelated to the proper text
#as.character(Pound)
Pound <- tm_map(Pound, stripWhitespace) # strip white space
#as.character(Pound)
Pound <- tm_map(Pound, stemDocument) # stem the document
#as.character(Pound)
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#as.character(Pound)
dtm <- DocumentTermMatrix(Pound) #making a document term matrix
dtm
dim(dtm)
freq <- colSums(as.matrix(dtm)) #finding the most frequent terms
length(freq)
ord <- order(freq)
freq[head(ord)] #taking a look at most frequent terma
freq[tail(ord)] #also at least frequent
head(table(freq),15) 
tail(table(freq),15)
plot(table(freq)) #taking a look at the distribution of frequency
dtm.ordered <- dtm[,order(freq, decreasing = T)] #ordering terms by frequency
findFreqTerms(dtm, lowfreq=20) 

freq <- sort(colSums(as.matrix(dtm)),decreasing=TRUE)
head(freq)

wf <- data.frame(word=names(freq), freq=freq) #plotting the results
head(wf)

subset(wf, freq>20) %>% #plotting terms that occur over 20 times
  ggplot (aes(word, freq)) +
  geom_bar (stat ="identity") +
  xlab("Term") + 
  ylab("Frequency") + 
  ggtitle("Most frequent terms in Ezra Pound's Personae")

library(wordcloud) #creating word cloud of results
set.seed(123)
wordcloud(names(freq), freq, max.words=100, colors=brewer.pal(6,"Dark2"))

dtm <- as.data.frame(as.matrix(dtm))
names(Pound)
write.csv(Pound,"Pound_Personae.csv") #writing to CSV

#Repeat same steps for Eliot collection, but this time I will use tm to convert collection to dtm and do all pre-processing in same move
dtm2 <- DocumentTermMatrix(Eliot,
           control = list(stopwords = TRUE,
                          tolower = TRUE,
                          removeNumbers = TRUE,
                          removePunctuation = TRUE,
                          stemming=TRUE))
dim(dtm2)
freq2 <- colSums(as.matrix(dtm2)) #finding the most frequent terms
length(freq2)
ord2 <- order(freq2)
freq2[head(ord2)] #taking a look at least frequent terms
freq2[tail(ord2)] #also at most frequent
head(table(freq2),15) 
tail(table(freq2),15)
plot(table(freq2)) #taking a look at the distribution of frequency
dtm2.ordered <- dtm2[,order(freq2, decreasing = T)] #ordering terms by frequency
findFreqTerms(dtm2, lowfreq=9) 

freq2 <- sort(colSums(as.matrix(dtm2)),decreasing=TRUE)
head(freq2)

wf2 <- data.frame(word=names(freq2), freq=freq2) #plotting the results
head(wf2)

subset(wf2, freq>9) %>% #plotting terms that occur over 9 times
  ggplot (aes(word, freq)) +
  geom_bar (stat ="identity") +
  xlab("Term") + 
  ylab("Frequency") + 
  ggtitle("Most frequent terms in T.S. Eliot's Prufrock")

library(wordcloud) #creating word cloud of results
set.seed(123)
wordcloud(names(freq2), freq2, max.words=100, colors=brewer.pal(7,"Dark2"))

dtm2 <- as.data.frame(as.matrix(dtm2))
names(Eliot)
write.csv(Eliot,"Eliot_Prufrock.csv") #writing to CSV

#Cleaning data for H.D.'s Sea Garden using same strategy
dtm3 <- DocumentTermMatrix(HD,
           control = list(stopwords = TRUE,
                          tolower = TRUE,
                          removeNumbers = TRUE,
                          removePunctuation = TRUE,
                          stemming=TRUE))
dim(dtm3)
freq3 <- colSums(as.matrix(dtm3)) #finding the most frequent terms
length(freq3)
ord3 <- order(freq3)
freq3[head(ord3)] #taking a look at most frequent terms
freq3[tail(ord3)] #also at least frequent
head(table(freq3),15) 
tail(table(freq3),15)
plot(table(freq3)) #taking a look at the distribution of frequency
#dtm3.ordered <- dtm3[,order(freq, decreasing = T)] #ordering terms by frequency
findFreqTerms(dtm3, lowfreq=12) 

freq3 <- sort(colSums(as.matrix(dtm3)),decreasing=TRUE) #ordering terms
head(freq3)

wf3 <- data.frame(word=names(freq3), freq=freq3) #plotting the results
head(wf3)

subset(wf3, freq>12) %>% #plotting terms that occur over 12 times
  ggplot (aes(word, freq)) +
  geom_bar (stat ="identity") +
  xlab("Term") + 
  ylab("Frequency") + 
  ggtitle("Most frequent terms in H.D.'s Sea Garden")

library(wordcloud) #creating word cloud of results
set.seed(123)
wordcloud(names(freq3), freq3, max.words=100, colors=brewer.pal(6,"Dark2"))

dtm3 <- as.data.frame(as.matrix(dtm3))
names(HD)
write.csv(HD,"HD_SeaGarden.csv") #writing to CSV




```


