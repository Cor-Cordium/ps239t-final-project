---
title: "Final Project III"
author: "Cory Merrill"
date: "December 11, 2015"
output: 
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


```{r}
#PART 3: I would like to write a regular expression that identifies/helps me identify instances of simile in each collection
#I also do some preparation for future work with POS tagging for these three colleciton to measure their grammatical entropy (complexity) 
#At the very end I share some code that is still in-progress for this project

rm(list=ls())
setwd("/Users/Orly/ps239t-final-project")
library(tm) # loading recommended libraries
library(RTextTools) 
library(qdap)
library(qdapDictionaries)
library(NLP)
library(openNLP)

#Import the data
collections<-Corpus(DirSource("Data/POS Tagging"))
collections <- tm_map(collections, content_transformer(tolower)) #coercing all to lowercase
#Write a regular expression that identifies instances of like or as in each collection
#I have restricted the expression to identify instances without enjambment (i.e. that falls on one line of a poem) for now 
HDlike <- sapply(collections$content[[1]],function(str) grep(" like",str,value=TRUE))
HDas <- sapply(collections$content[[1]],function(str) grep(" as ",str,value=TRUE))
Poundlike <- sapply(collections$content[[2]],function(str) grep(" like",str,value=TRUE))
Poundas <- sapply(collections$content[[2]],function(str) grep(" as ",str,value=TRUE))
Eliotlike<- sapply(collections$content[[3]],function(str) grep(" like",str,value=TRUE))
Eliotas <- sapply(collections$content[[3]],function(str) grep(" as ",str,value=TRUE))

#combine two cases (like and as) for each author
HDsimile <- c(HDlike, HDas)
Poundsimile <-c(Poundlike, Poundas)
Eliotsimile <-c(Eliotlike, Eliotas)

HDsimile #taking a look at results
Poundsimile
Eliotsimile

#To do: subset from these lists so that instances of non-figurative uses of as and like are removed.


#POS Tagging for the collections
head(as.character(collections$content[[2]])) #checking the order
#I am curious to see how sentence detection will work when applied to these collections of poetry
# Combining sections of text for each collection
HD <- paste(collections$content[[1]], collapse = " ")
Pound <-paste(collections$content[[2]], collapse = " ")
Eliot <-paste(collections$content[[3]], collapse = " ")

# Split into sentences
sentencesE <- sent_detect(Eliot)
sentencesH <- sent_detect(HD)
sentencesP <- sent_detect(Pound)

posdE <- pos(sentencesE)
posdH <- pos(sentencesH)
posdP <- pos(sentencesP)

# taking a peek at the results
#posdE

#looking at sentences with tags in-line, list of tags by sentence, & word counts
#posdE$POStagged
dim(posdE$POStagged)
posdE$POStagged[,1] #isolating vector of tagged sentences, 

#visualizing POS for each collection
plot(posdE)
plot(posdH)
plot(posdP)
#getting list of tags for each sentence
posdE.list <- posdE[[2]][[2]]
posdH.list <- posdH[[2]][[2]]
posdP.list <- posdP[[2]][[2]]

posdE.list[1:5]


#Analyze for grammatical entropy (how much of a hot mess each collection is ;-)
# Shorten POS-tags to two characters
posdE.list<-lapply(posdE.list, function(x) sapply(x, function(x) substr(x,1,2)))
posdH.list<-lapply(posdH.list, function(x) sapply(x, function(x) substr(x,1,2)))
posdP.list<-lapply(posdP.list, function(x) sapply(x, function(x) substr(x,1,2)))

# Get bigrams of POS tags
posE.grams <- lapply(posdE.list, function(x) ngrams(paste(x, collapse = " "), 2))
posH.grams <- lapply(posdH.list, function(x) ngrams(paste(x, collapse = " "), 2))
posP.grams <- lapply(posdP.list, function(x) ngrams(paste(x, collapse = " "), 2))

# Get the list of bigrams for each sentence
gramE.list <-sapply(posE.grams, function(x) x$all_n$n_2)
gramH.list <-sapply(posH.grams, function(x) x$all_n$n_2)
gramP.list <-sapply(posP.grams, function(x) x$all_n$n_2)


# Pull out all the bigrams into a single list, since we no longer are interested in sentence divisions
gramE.listu <- unlist(gramE.list, recursive=FALSE)
gramH.listu <- unlist(gramH.list, recursive=FALSE)
gramP.listu <- unlist(gramP.list, recursive=FALSE)

#gramE.listu

# Join bigrams with a hyphen into a single unit
gramE.listu<-sapply(gramE.listu, function(x) paste(x, sep=' ',collapse = '-'))
gramH.listu<-sapply(gramH.listu, function(x) paste(x, sep=' ',collapse = '-'))
gramP.listu<-sapply(gramP.listu, function(x) paste(x, sep=' ',collapse = '-'))

# Count the instances of all bigram units
gramE.counts <- as.vector(table(gramE.listu))
gramH.counts <- as.vector(table(gramH.listu))
gramP.counts <- as.vector(table(gramP.listu))

# Take a look at the counts
gramE.counts
gramH.counts
gramP.counts

#calculating degree of grammatical entropy (complexity) for each collection
# Get the probability of randomly choosing a particular bigram from the text
probE<-gramE.counts/sum(gramE.counts)
probH<-gramH.counts/sum(gramH.counts)
probP<-gramP.counts/sum(gramP.counts)


# Calculate the Shannon entropy of the collections
ShannonE.entropy <- -sum(log2(probE)*probE)
ShannonH.entropy <- -sum(log2(probH)*probH)
ShannonP.entropy <- -sum(log2(probP)*probP)
ShannonE.entropy
ShannonH.entropy
ShannonP.entropy

```


